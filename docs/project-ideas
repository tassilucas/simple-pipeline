
I like programming and soccer so, I decided to create this project as a way of learning more about
Apache Airflow. I will be developing a complete pipeline that ingest, load and transform data
from soccer matches (only Brazil) to be used in BI dashboards.

1. Data ingestion:
    . API used: https://www.api-football.com/
    . This API gives the information needed in JSON format but has some limitations:
    	- Only 100 requests / day for free plan,
	  meaning the pipeline will be runned in limited daily batch
	- 10 requests / minute
    . Some useful endpoints:
    	- /teams/statistics - information about team stats like wins, goals (home, away).
	- /players/statistics - information about player stats like goals, shots, games, photo, etc.
	- /leagues - information about leagues

1.1 Data ingestion storing:
    . Data should be ingested and stored in Amazon S3.

2. Data transform:
    . Normalize data and check formats (date, lower/upper case, etc). 
    . Pydantic: data validation.

3. Data loading:
    . The result data will be loaded in Data Warehouse.
      . Amazon Redshift

4. Data architecture (?)
    . Still need to figure-out but probably
    	API-Football -> S3 -> Redshift
    <all tasks orchestrated by Apache Airflow>
    . Will be stored in folder /teams and /players. Each file contains a team information in JSON object.

5. Airflow deployment
    . Docker
    . Celery Workers?
    . Which more learned concepts of Airflow can we explore with this project?
    	- XCom
	- Sensors
	- Paralell API requests (via Dynamic Mapping)
